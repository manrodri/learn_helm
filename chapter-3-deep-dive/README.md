# Beyond the Basics with Helm

## Templating and Dry Runs

Roughly, then, the process is:

1. Load the entire chart, including its dependencies.
1. Parse the values.
1. Execute the templates, generating YAML.
1. Parse the YAML into Kubernetes objects to verify the data.
1. Send it to Kubernetes.

### Example

```
$ helm install mysite bitnami/drupal --set drupalUsername=admin
```

* In the first phase, Helm will locate the chart named bitnami/drupal and load that chart. If is local fine, if is remote you need a repository configure.

* Then it will transform --set drupalUsername=admin into a value that can be injected into the templates.

* At this point, Helm will read all of the templates in the Drupal chart, and then execute those templates, passing the merged values into the template engine. It is important to note that, when executed,*some Helm templates* require information about Kubernetes. So during template rendering, Helm *may contact the Kubernetes API server*. 

* The output of the preceding step is then parsed from YAML into Kubernetes objects. Helm will perform some schema-level validation at this point, making sure that the objects are well-formed.

* Helm sends the YAML data to the Kubernetes API server. This is the server that kubectl and other Kubernetes tools interact with.

## The --dry-run Flag

It will cause Helm to step through the first four phases (load the chart, determine the values, render the templates, format to YAML)

```

manuel@dev-server:~/helm/chapter-3$ helm install mysite bitnami/drupal --values values.yaml --set drupalEmail=foo@example.com --dry-run
NAME: mysite
LAST DEPLOYED: Wed Sep  1 06:43:04 2021
NAMESPACE: default
STATUS: pending-install
REVISION: 1
TEST SUITE: None
HOOKS:
MANIFEST:
---
# Source: drupal/charts/mariadb/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mysite-mariadb
  namespace: default
  labels:
    app.kubernetes.io/name: mariadb
    helm.sh/chart: mariadb-9.4.4
    app.kubernetes.io/instance: mysite
    app.kubernetes.io/managed-by: Helm
  annotations:
---
# Source: drupal/charts/mariadb/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:

...

```

The principal purpose of the --dry-run flag is to give people a chance to inspect and debug output before sending it on to Kubernetes. 

*--dry-run can cause a few problems:*

1. --dry-run mixes non-YAML information with the rendered templates.
1. A --dry-run on upgrade can produce different YAML output than a --dry-run on install, and this can be confusing.
1. It contacts the Kubernetes API server for validation, which means Helm has to have Kubernetes credentials even if it is just used to --dry-run a release.
1. It also inserts information into the template engine that is cluster-specific. Because of this, the output of some rendering processes may be cluster-specific.


## The helm template Command

While --dry-run is designed for debugging, helm template is designed to isolate the template rendering process of Helm from the installation or upgrade logic.

* During helm template, Helm never contacts a remote Kubernetes server.
* The template command always acts like an installation.
* Template functions and directives that would normally require contacting a Kubernetes server will instead only return default data.
* The chart only has access to default Kubernetes kinds.


Regarding the last item, helm template makes a notable simplifying assumption. Kubernetes servers support built-in kinds (Pod, Service, ConfigMap, and so on) as well as custom kinds generated by custom resource definitions (CRDs). When Helm is compiled, it is compiled against a particular version of Kubernetes. The Kubernetes libraries contain the list of built-in kinds for that release.

Running an old version of Helm against a chart that uses new kinds or versions can produce an error during helm template because Helm will not have the newest kinds or versions compiled into it.


The output is also different from --dry-run.
 * only the YAML-formatted Kubernetes manifest is printed by default.
 * Because Helm does not contact a Kubernetes cluster during a helm template run, it does not do complete validation of the output. You may choose to use the --validate flag if you want that behavior, but in this case Helm will need a valid kubeconfig file with credentials for a cluster.



